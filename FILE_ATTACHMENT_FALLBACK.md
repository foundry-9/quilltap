# File Attachment Fallback System

## Overview

The File Attachment Fallback system automatically handles file attachments for providers that don't support them natively. This ensures users can attach files regardless of which LLM provider they're using.

## How It Works

When a file is attached to a chat with a provider that doesn't support that file type:

### 1. Text Files
Text files (`.txt`, `.md`, `.csv`, `.json`, etc.) are automatically converted to inline text within the message:

```
[User attached text file: example.txt]

... file contents here ...

[End of attached file]

Original user message continues here...
```

The text is prepended to the user's message, allowing text-only providers to "see" the file content.

### 2. Image Files
Images are sent to the **Cheap LLM** (if configured and supports images) to generate a detailed description:

1. System checks if cheap LLM supports images
2. Image is sent with prompt: *"Please describe this image in great detail..."*
3. Description is received from cheap LLM
4. Description is formatted and prepended to user's message:

```
[Image: photo.jpg]

Image Description (generated by AI):
A photograph showing a sunset over the ocean...

Original user message continues here...
```

### 3. Unsupported Files
If a file type cannot be processed (not text or image, or image but no vision-capable cheap LLM):
- Error message is logged
- User sees a note in the stream about the unsupported file
- Chat continues without the attachment

## Supported Providers

### Providers with Native File Support:
- **OpenAI**: Images (JPEG, PNG, GIF, WebP)
- **Anthropic**: Images + PDF documents
- **Google**: Images (JPEG, PNG, GIF, WebP)
- **Grok**: Images + PDF + Text files

### Providers Requiring Fallback:
- **Ollama**: No native file support (all files use fallback)
- **OpenRouter**: No native file support (depends on proxied model)
- **OpenAI Compatible**: No native file support (varies by implementation)
- **Gab AI**: No native file support

## Implementation Details

### Backend (`lib/chat/file-attachment-fallback.ts`)

**Key Functions:**
- `needsFallbackProcessing()` - Checks if file needs processing
- `convertTextFileToInline()` - Converts text files to inline content
- `generateImageDescription()` - Uses cheap LLM to describe images
- `processFileAttachmentFallback()` - Main processor
- `formatFallbackAsMessagePrefix()` - Formats results for message

**Processing Flow:**
```typescript
// In app/api/chats/[id]/messages/route.ts

1. User sends message with file attachments
2. Files are loaded from storage
3. For each attachment:
   - Check if provider supports this MIME type
   - If not, process via fallback:
     * Text file → read and format as inline text
     * Image file → send to cheap LLM for description
     * Other → mark as unsupported
4. Prepend fallback content to user's message
5. Filter out processed attachments (don't send as native attachments)
6. Continue normal chat flow
```

### Stream Events

The message API sends a `fileProcessing` event in the stream:

```json
{
  "fileProcessing": [
    {
      "filename": "example.png",
      "type": "image_description",
      "usedCheapLLM": true,
      "error": null
    }
  ]
}
```

**Event Types:**
- `text` - Text file converted to inline
- `image_description` - Image described by cheap LLM
- `unsupported` - File could not be processed

### Frontend

The frontend receives the `fileProcessing` event and can display it to users. Currently, the processing happens synchronously before the stream starts, so users experience it as a brief delay before the response begins.

**Future Enhancement:** Display real-time progress for image descriptions.

## Configuration

### Cheap LLM Setup
For image fallback to work, you need a cheap LLM profile that supports images:

1. Go to Settings → Connection Profiles
2. Create a profile with a vision-capable model:
   - **Anthropic**: claude-haiku-4-5-20251015
   - **OpenAI**: gpt-4o-mini
   - **Google**: gemini-2.0-flash
   - **Grok**: grok-vision-beta
3. Mark it as "cheap" or set as default cheap profile in Chat Settings

### Fallback Behavior

**No Cheap LLM Configured:**
- Text files still work (converted to inline)
- Images cannot be processed → unsupported error

**Cheap LLM Without Vision:**
- Text files still work
- Images cannot be processed → unsupported error

**Vision-Capable Cheap LLM:**
- Text files work (inline conversion)
- Images work (AI description)

## Examples

### Example 1: Text File with Ollama
```
User: [Attaches notes.txt] Can you summarize these notes?

Provider: Ollama (no file support)

System processes:
- Detects Ollama doesn't support text files
- Reads notes.txt content
- Prepends to message

Final message to LLM:
[User attached text file: notes.txt]

Meeting Notes - Q4 Planning
- Revenue targets: $2M
- Team expansion: 3 new hires
...

[End of attached file]

Can you summarize these notes?
```

### Example 2: Image with OpenRouter
```
User: [Attaches screenshot.png] What's wrong with this code?

Provider: OpenRouter (no file support)
Cheap LLM: GPT-4o-mini (supports images)

System processes:
- Detects OpenRouter doesn't support images
- Sends screenshot.png to GPT-4o-mini with description prompt
- Receives: "The screenshot shows Python code with a syntax error on line 23..."
- Prepends to message

Final message to LLM:
[Image: screenshot.png]

Image Description (generated by AI):
The screenshot shows Python code with a syntax error on line 23.
The function definition is missing a colon after the parameter list...

What's wrong with this code?
```

### Example 3: Mixed Files with Gab AI
```
User: [Attaches diagram.png, config.json] Review this setup

Provider: Gab AI (no file support)
Cheap LLM: Claude Haiku (supports images)

System processes:
File 1 (diagram.png):
- Sends to Claude Haiku for description
- Gets detailed description of architecture diagram

File 2 (config.json):
- Reads JSON file content
- Formats as inline text

Final message to LLM:
[Image: diagram.png]

Image Description (generated by AI):
The diagram shows a three-tier architecture with a frontend React app...

[User attached text file: config.json]

{
  "database": "postgresql",
  "port": 5432,
  ...
}

[End of attached file]

Review this setup
```

## Error Handling

**Common Errors:**
- "No cheap LLM profile available for image description"
- "Cheap LLM profile does not support image files"
- "Failed to read text file: [reason]"
- "Failed to generate image description: [reason]"

**Graceful Degradation:**
- Errors don't stop the chat
- Unsupported files are noted but message still sends
- Users can retry or use a different provider

## Performance Considerations

**Text Files:**
- Very fast (simple file read + string concat)
- No API calls
- Minimal latency

**Image Files:**
- Requires API call to cheap LLM
- Latency: typically 1-3 seconds
- Cost: Uses cheap LLM credits
- Users see brief delay before response starts

**Optimization Tips:**
- Use local Ollama for cheap LLM (free, but no vision)
- Use fast vision models (GPT-4o-mini, Claude Haiku, Gemini Flash)
- Keep images under 1MB for faster processing

## Testing

To test the fallback system:

1. **Text File Fallback:**
   - Create Ollama profile
   - Attach .txt file to chat
   - Verify content appears inline in debug panel

2. **Image Description Fallback:**
   - Create Gab AI or OpenRouter profile (no image support)
   - Set up cheap LLM with vision (GPT-4o-mini)
   - Attach image to chat
   - Verify description is generated and prepended

3. **Error Handling:**
   - Remove cheap LLM
   - Try attaching image to non-vision provider
   - Verify graceful error message

## Future Enhancements

- [ ] Real-time progress UI for image descriptions
- [ ] PDF text extraction fallback
- [ ] Audio file transcription fallback
- [ ] Caching of image descriptions to avoid re-processing
- [ ] User preference for fallback behavior (disable, always use, etc.)
- [ ] Parallel processing of multiple image descriptions
- [ ] Thumbnail preview in description for verification

## Related Files

- `lib/chat/file-attachment-fallback.ts` - Main fallback logic
- `app/api/chats/[id]/messages/route.ts` - Integration point (lines 386-414)
- `lib/llm/attachment-support.ts` - Provider capability detection
- `lib/llm/connection-profile-utils.ts` - Profile utilities
- `lib/llm/cheap-llm.ts` - Cheap LLM selection logic
