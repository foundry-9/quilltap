# Quilltap AI Chat Platform - Implementation Roadmap

> **Last Updated**: 2025-11-17
> **Version**: 2.0 (Revised with Next.js, OAuth, simplified stack)

## Executive Summary

Quilltap is a Docker-containerized Next.js web application for AI-powered roleplay chat, supporting multiple LLM providers with SillyTavern compatibility. Built with modern best practices, OAuth authentication, and a streamlined technology stack for rapid development.

---

## Technology Stack

### Full-Stack Framework

- **Framework**: Next.js 14+ (App Router)
- **Runtime**: Node.js 20 LTS
- **Language**: TypeScript
- **API Routes**: Next.js API Routes (replaces Express)
- **Frontend**: React 18 (built into Next.js)
- **Styling**: Tailwind CSS + shadcn/ui components

### Database & Storage

- **Database**: PostgreSQL 16
- **ORM**: Prisma
- **Caching**: None initially (add Redis post-1.0 if needed)

### Authentication

- **Auth Framework**: NextAuth.js v5 (Auth.js)
- **Providers**:
  - Google OAuth (v1.0)
  - Apple OAuth (post-1.0)
  - GitHub OAuth (post-1.0)
- **Session**: JWT in httpOnly cookies

### Testing

- **Unit Tests**: Jest + React Testing Library
- **Integration Tests**: Playwright
- **API Tests**: Next.js test utilities

### Infrastructure

- **Container**: Docker + Docker Compose
- **Web Server**: Nginx (reverse proxy + SSL termination)
- **SSL**: Let's Encrypt via Certbot (automated renewal)
- **Build**: Multi-stage Docker builds

### Key Dependencies

```json
{
  "dependencies": {
    "next": "^14.0.0",
    "react": "^18.0.0",
    "next-auth": "^5.0.0",
    "@prisma/client": "^5.0.0",
    "@ai-sdk/openai": "^0.0.0",
    "@anthropic-ai/sdk": "^0.20.0",
    "zod": "^3.22.0",
    "tailwindcss": "^3.4.0"
  }
}
```

---

## Why These Technology Choices?

### Next.js vs Express + Vite

**Next.js is significantly better for this project:**

**Pros:**

- **Single codebase**: API routes + frontend in one project
- **Built-in optimizations**: SSR, code splitting, image optimization out of the box
- **Faster development**: No CORS issues, shared types between frontend/backend
- **Better DX**: Hot reload for everything, integrated routing
- **Production-ready**: Vercel optimized it heavily for performance
- **Smaller Docker image**: One build instead of two

**Cons:**

- Slightly less flexible than Express for complex middleware (but 99% of use cases are fine)
- Opinionated structure (but that's actually good for speed)

**Verdict**: Use Next.js. You'll save weeks of setup and configuration time.

### Why No Redis (Initially)?

**Redis is typically used for:**

- Session storage (but Next.js + JWT in cookies works fine)
- Caching LLM responses (maybe useful later)
- Rate limiting (can do this in-memory for MVP)
- Real-time features (WebSocket state)

**Recommendation:**

- **Phase 0-0.5**: Skip Redis entirely. Use JWT in httpOnly cookies for auth.
- **Phase 0.7+**: Add Redis IF you need:
  - Response caching (expensive LLM calls)
  - Rate limiting across multiple server instances
  - WebSocket session management

**Verdict**: Drop Redis from initial plan. Add it later if you actually need it. This simplifies your Docker setup significantly.

### Node.js SSL vs Nginx

**Nginx is better for production:**

**Direct Node.js SSL Problems:**

- Node needs to run as root to bind to port 443 (security risk)
- No automatic cert renewal handling
- Less battle-tested for SSL/TLS edge cases
- Harder to do advanced routing/load balancing later

**Nginx Benefits:**

- Handles SSL termination (Node just runs on port 3000)
- Certbot integration is seamless
- Better performance for static assets
- Can add caching, compression, rate limiting without touching Node
- Industry standard (lots of docs/support)

**Verdict**: Use Nginx for production. For local dev, just use `http://localhost:3000`.

### OAuth + Encryption Key Derivation

**The Brilliant Part:**
Using OAuth means:

- No password management headaches
- Better UX (one-click login)
- Google/Apple/GitHub handle 2FA, recovery, etc.
- Can derive encryption keys from OAuth identity

**The Critical Security Decision:**

We use **per-user encryption** with keys derived from stable user IDs:

```javascript
// Per-user encryption
1. OAuth login (Google, later Apple/GitHub)
2. Create user record with user.id
3. Derive encryption key from: PBKDF2(user.id + MASTER_PEPPER)
4. Encrypt API keys with user-specific key
5. Store encrypted keys in database
```

**Why this works:**

- Each user has their own encryption key
- Derived from stable user ID (not session tokens)
- Works with any OAuth provider
- You control the master pepper (backup recovery possible)
- Simple to implement and reason about

**Post-1.0 enhancement:**
Add client-side encryption where user enters a "vault password" that ONLY they know, giving true zero-knowledge encryption. But that's overkill for v1.

---

## Architecture Overview

```text
┌─────────────────────────────────────────────┐
│      Nginx (SSL Termination - Prod)        │
└─────────────┬───────────────────────────────┘
              │
┌─────────────▼───────────────────────────────┐
│            Next.js Application              │
│  ┌──────────────────────────────────────┐   │
│  │   Pages   │  API Routes  │  Server  │   │
│  │ (React)   │  (Backend)   │ Actions  │   │
│  └──────────────────────────────────────┘   │
│  ┌──────────────────────────────────────┐   │
│  │  NextAuth │  LLM Svc │  Encryption  │   │
│  └──────────────────────────────────────┘   │
└─────────────┬───────────────────────────────┘
              │
        ┌─────▼─────┐
        │PostgreSQL │
        └───────────┘
```

---

## Milestone Roadmap

### Phase 0: Foundation (Week 1)

**Goal**: Project setup with OAuth authentication

**Tasks:**

- [x] Initialize Next.js 14 project with TypeScript
- [x] Configure Prisma with PostgreSQL
- [x] Set up Docker Compose (dev environment)
- [x] Configure NextAuth.js with Google OAuth
- [x] Basic user model and database schema
- [x] Environment variable setup
- [x] Tailwind CSS + shadcn/ui setup
- [x] Basic layout and navigation

**Deliverable**: User can sign in with Google and see a dashboard

**Status**: ✅ **COMPLETE** (November 17, 2025)

**Completion Notes**:

- Next.js 14 application fully set up with TypeScript
- Prisma ORM configured with PostgreSQL database
- Docker Compose development environment ready
- Google OAuth authentication working via NextAuth.js
- User model and authentication tables created
- Environment variables configured with .env.example template
- Tailwind CSS integrated with custom theme
- Dashboard with navigation and user profile display
- Landing page with authentication flow
- README with complete setup instructions

All Phase 0 deliverables met. Users can successfully sign in with Google OAuth and access a protected dashboard.

---

### Phase 0.3: Core Infrastructure (Week 2)

**Goal**: API key management and encryption

**Tasks:**

- [x] Encryption service (AES-256-GCM, per-user keys)
- [x] API key CRUD operations
- [x] API key storage (encrypted in PostgreSQL)
- [x] API key testing endpoints
- [x] Connection profile management
- [x] Unit tests for encryption
- [x] Basic error handling middleware

**Deliverable**: Users can securely store and manage API keys

**Status**: ✅ **COMPLETE** (November 17, 2025)

**Completion Notes**:

- Encryption service implemented with AES-256-GCM per-user encryption
- User-specific keys derived from user ID + master pepper using PBKDF2
- API key CRUD operations available at `/api/keys`
- Individual API key operations at `/api/keys/[id]`
- API key validation endpoint at `/api/keys/[id]/test`
- Connection profile CRUD operations at `/api/profiles`
- Individual profile operations at `/api/profiles/[id]`
- Comprehensive unit tests for encryption service
- Error handling middleware with security headers
- Database schema updated with ApiKey and ConnectionProfile models
- Migration SQL created for database updates
- Support for 5 LLM providers: OpenAI, Anthropic, Ollama, OpenRouter, OpenAI-compatible
- Secure key masking for UI display
- Authentication verification on all endpoints

All Phase 0.3 deliverables met. Users can now securely store, manage, and test encrypted API keys with connection profiles.

---

### Phase 0.5: Single Chat MVP (Weeks 3-4)

**Goal**: Working chat with one LLM provider

**Tasks:**

- [ ] Character model and CRUD API
- [ ] Basic character creation UI
- [ ] OpenAI integration service
- [ ] Chat model and message storage
- [ ] Simple chat interface
- [ ] Real-time streaming responses (Server-Sent Events)
- [ ] Chat initialization with character context
- [ ] Message history display
- [ ] Integration tests for chat flow

**Deliverable**: Users can create a character and chat using OpenAI

**Status**: ⏳ **PENDING**

---

### Phase 0.7: Multi-Provider Support (Weeks 5-6)

**Goal**: All LLM providers working

**Tasks:**

- [ ] Abstract LLM provider interface
- [ ] Anthropic integration
- [ ] OpenAI-compatible provider (generic)
- [ ] Ollama integration
- [ ] OpenRouter integration
- [ ] Provider selection in connection profiles
- [ ] Model listing per provider
- [ ] Error handling for each provider
- [ ] Provider switching in UI

**Deliverable**: Users can chat using any of the 5 provider types

**Status**: ⏳ **PENDING**

---

### Phase 0.9: Feature Complete (Weeks 7-9)

**Goal**: All core features implemented

**Tasks:**

- [ ] Persona system (character-linked and chat-linked)
- [ ] Multiple characters management
- [ ] Advanced chat initialization (character + persona + scenario)
- [ ] SillyTavern character import (PNG + JSON)
- [ ] SillyTavern character export
- [ ] SillyTavern persona import/export
- [ ] SillyTavern chat import/export
- [ ] Message editing and deletion
- [ ] Chat branching/swipes (alternative responses)
- [ ] UI polish and responsive design

**Deliverable**: Full feature parity with requirements, ST compatibility

**Status**: ⏳ **PENDING**

---

### Phase 1.0: Production Ready (Weeks 10-11)

**Goal**: Production deployment ready

**Tasks:**

- [ ] Production Docker Compose with Nginx
- [ ] SSL certificate automation (Certbot)
- [ ] Comprehensive test coverage (>80%)
- [ ] Performance optimization
- [ ] Security audit
- [ ] Rate limiting
- [ ] Logging and monitoring setup
- [ ] Documentation (deployment guide, API docs)
- [ ] Environment variable validation
- [ ] Database backup strategy

**Deliverable**: v1.0 release, production-ready deployment

**Status**: ⏳ **PENDING**

---

### Post-1.0: Enhancement Backlog

**Future features** (prioritize based on user feedback):

- World Book/Lorebook support
- Redis caching for LLM responses
- Apple and GitHub OAuth providers
- Advanced prompt templates
- Chat folders/organization
- Image generation integration
- Voice/TTS integration
- Mobile-responsive PWA
- Multi-user shared chats
- Admin dashboard
- Usage analytics
- Export to other formats (Character.AI, etc.)

---

## Database Schema

### Core Tables

#### users

```prisma
model User {
  id            String    @id @default(uuid())
  email         String    @unique
  name          String?
  image         String?
  emailVerified DateTime?
  createdAt     DateTime  @default(now())
  updatedAt     DateTime  @updatedAt

  accounts      Account[]
  apiKeys       ApiKey[]
  profiles      ConnectionProfile[]
  characters    Character[]
  personas      Persona[]
  chats         Chat[]
}
```

#### accounts (NextAuth.js OAuth)

```prisma
model Account {
  id                String  @id @default(uuid())
  userId            String
  type              String
  provider          String
  providerAccountId String
  refresh_token     String?
  access_token      String?
  expires_at        Int?
  token_type        String?
  scope             String?
  id_token          String?
  session_state     String?

  user User @relation(fields: [userId], references: [id], onDelete: Cascade)

  @@unique([provider, providerAccountId])
}
```

#### api_keys (encrypted at rest)

```prisma
model ApiKey {
  id            String   @id @default(uuid())
  userId        String
  provider      Provider
  label         String
  keyEncrypted  String   // AES-256-GCM encrypted
  keyIv         String   // Initialization vector
  keyAuthTag    String   // Authentication tag
  isActive      Boolean  @default(true)
  lastUsed      DateTime?
  createdAt     DateTime @default(now())
  updatedAt     DateTime @updatedAt

  user          User     @relation(fields: [userId], references: [id], onDelete: Cascade)
  profiles      ConnectionProfile[]
}

enum Provider {
  OPENAI
  ANTHROPIC
  OLLAMA
  OPENROUTER
  OPENAI_COMPATIBLE
}
```

#### connection_profiles

```prisma
model ConnectionProfile {
  id              String   @id @default(uuid())
  userId          String
  name            String
  provider        Provider
  apiKeyId        String?
  baseUrl         String?  // For Ollama/OpenAI-compatible
  modelName       String
  parameters      Json     // { temperature, max_tokens, top_p, etc. }
  isDefault       Boolean  @default(false)
  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt

  user            User     @relation(fields: [userId], references: [id], onDelete: Cascade)
  apiKey          ApiKey?  @relation(fields: [apiKeyId], references: [id], onDelete: SetNull)
  chats           Chat[]
}
```

#### characters

```prisma
model Character {
  id                String   @id @default(uuid())
  userId            String
  name              String
  description       String   @db.Text
  personality       String   @db.Text
  scenario          String   @db.Text
  firstMessage      String   @db.Text
  exampleDialogues  String?  @db.Text
  systemPrompt      String?  @db.Text
  avatarUrl         String?
  sillyTavernData   Json?    // Full ST character spec
  createdAt         DateTime @default(now())
  updatedAt         DateTime @updatedAt

  user              User     @relation(fields: [userId], references: [id], onDelete: Cascade)
  chats             Chat[]
  personas          CharacterPersona[]
}
```

#### personas

```prisma
model Persona {
  id                String   @id @default(uuid())
  userId            String
  name              String
  description       String   @db.Text
  personalityTraits String?  @db.Text
  avatarUrl         String?
  sillyTavernData   Json?    // Full ST persona spec
  createdAt         DateTime @default(now())
  updatedAt         DateTime @updatedAt

  user              User     @relation(fields: [userId], references: [id], onDelete: Cascade)
  characters        CharacterPersona[]
  chats             Chat[]
}
```

#### character_personas (many-to-many)

```prisma
model CharacterPersona {
  characterId String
  personaId   String
  isDefault   Boolean @default(false)

  character   Character @relation(fields: [characterId], references: [id], onDelete: Cascade)
  persona     Persona   @relation(fields: [personaId], references: [id], onDelete: Cascade)

  @@id([characterId, personaId])
}
```

#### chats

```prisma
model Chat {
  id                  String   @id @default(uuid())
  userId              String
  characterId         String
  personaId           String?
  connectionProfileId String
  title               String
  contextSummary      String?  @db.Text
  sillyTavernMetadata Json?
  createdAt           DateTime @default(now())
  updatedAt           DateTime @updatedAt

  user                User              @relation(fields: [userId], references: [id], onDelete: Cascade)
  character           Character         @relation(fields: [characterId], references: [id], onDelete: Cascade)
  persona             Persona?          @relation(fields: [personaId], references: [id], onDelete: SetNull)
  connectionProfile   ConnectionProfile @relation(fields: [connectionProfileId], references: [id])
  messages            Message[]
}
```

#### messages

```prisma
model Message {
  id            String    @id @default(uuid())
  chatId        String
  role          Role
  content       String    @db.Text
  rawResponse   Json?     // Full LLM response
  tokenCount    Int?
  swipeGroupId  String?   // For alternative responses
  swipeIndex    Int?
  createdAt     DateTime  @default(now())

  chat          Chat      @relation(fields: [chatId], references: [id], onDelete: Cascade)

  @@index([chatId, createdAt])
  @@index([swipeGroupId])
}

enum Role {
  SYSTEM
  USER
  ASSISTANT
}
```

---

## API Structure (Next.js App Router)

### Endpoint Organization

```text
app/api/
├── auth/
│   └── [...nextauth]/route.ts       # NextAuth handlers
├── keys/
│   ├── route.ts                     # GET, POST /api/keys
│   └── [id]/
│       ├── route.ts                 # PUT, DELETE /api/keys/:id
│       └── test/route.ts            # POST /api/keys/:id/test
├── profiles/
│   ├── route.ts                     # GET, POST /api/profiles
│   └── [id]/route.ts                # GET, PUT, DELETE /api/profiles/:id
├── characters/
│   ├── route.ts                     # GET, POST /api/characters
│   ├── [id]/
│   │   ├── route.ts                 # GET, PUT, DELETE /api/characters/:id
│   │   └── export/route.ts          # GET /api/characters/:id/export
│   └── import/route.ts              # POST /api/characters/import
├── personas/
│   ├── route.ts
│   ├── [id]/route.ts
│   └── import/route.ts
├── chats/
│   ├── route.ts                     # GET, POST /api/chats
│   ├── [id]/
│   │   ├── route.ts                 # GET, PUT, DELETE /api/chats/:id
│   │   ├── messages/route.ts        # GET, POST /api/chats/:id/messages
│   │   └── export/route.ts
│   └── import/route.ts
└── messages/
    └── [id]/
        ├── route.ts                 # PUT, DELETE /api/messages/:id
        └── swipe/route.ts           # POST /api/messages/:id/swipe
```

---

## Security Implementation

### OAuth Authentication (NextAuth.js)

```typescript
// app/api/auth/[...nextauth]/route.ts
import NextAuth from "next-auth"
import GoogleProvider from "next-auth/providers/google"
import { PrismaAdapter } from "@auth/prisma-adapter"
import { prisma } from "@/lib/prisma"

export const authOptions = {
  adapter: PrismaAdapter(prisma),
  providers: [
    GoogleProvider({
      clientId: process.env.GOOGLE_CLIENT_ID!,
      clientSecret: process.env.GOOGLE_CLIENT_SECRET!,
    }),
    // Post-1.0:
    // AppleProvider({ ... }),
    // GitHubProvider({ ... }),
  ],
  callbacks: {
    session({ session, user }) {
      session.user.id = user.id
      return session
    },
  },
  pages: {
    signIn: '/signin',
    error: '/error',
  },
}

const handler = NextAuth(authOptions)
export { handler as GET, handler as POST }
```

### Encryption Strategy (Per-User Keys)

```typescript
// lib/encryption.ts
import crypto from 'crypto'

const ALGORITHM = 'aes-256-gcm'
const MASTER_PEPPER = process.env.ENCRYPTION_MASTER_PEPPER!

/**
 * Derive a user-specific encryption key
 * Key is derived from user ID + master pepper
 * This allows user to access their keys via OAuth login
 */
function deriveUserKey(userId: string): Buffer {
  return crypto.pbkdf2Sync(
    userId,
    MASTER_PEPPER,
    100000, // iterations
    32,     // key length
    'sha256'
  )
}

/**
 * Encrypt API key with user-specific key
 */
export function encryptApiKey(apiKey: string, userId: string) {
  const key = deriveUserKey(userId)
  const iv = crypto.randomBytes(16)
  const cipher = crypto.createCipheriv(ALGORITHM, key, iv)

  let encrypted = cipher.update(apiKey, 'utf8', 'hex')
  encrypted += cipher.final('hex')

  const authTag = cipher.getAuthTag()

  return {
    encrypted,
    iv: iv.toString('hex'),
    authTag: authTag.toString('hex'),
  }
}

/**
 * Decrypt API key with user-specific key
 */
export function decryptApiKey(
  encrypted: string,
  iv: string,
  authTag: string,
  userId: string
): string {
  const key = deriveUserKey(userId)
  const decipher = crypto.createDecipheriv(
    ALGORITHM,
    key,
    Buffer.from(iv, 'hex')
  )

  decipher.setAuthTag(Buffer.from(authTag, 'hex'))

  let decrypted = decipher.update(encrypted, 'hex', 'utf8')
  decrypted += decipher.final('utf8')

  return decrypted
}
```

**Security Features:**

- ✅ Per-user encryption keys (not shared)
- ✅ Derived from stable user ID (works across sessions)
- ✅ AES-256-GCM (authenticated encryption)
- ✅ Random IV per encrypted value
- ✅ Master pepper in environment variable only
- ✅ Keys never logged or exposed in API responses
- ✅ OAuth ensures only authorized user can access their data

### SSL/TLS Configuration

**Development**: Plain HTTP on localhost

```bash
npm run dev
# Access at http://localhost:3000
```

**Production**: Nginx with Let's Encrypt

```nginx
# docker/nginx.conf
server {
    listen 80;
    server_name yourdomain.com;

    location /.well-known/acme-challenge/ {
        root /var/www/certbot;
    }

    location / {
        return 301 https://$host$request_uri;
    }
}

server {
    listen 443 ssl http2;
    server_name yourdomain.com;

    ssl_certificate /etc/nginx/certs/live/yourdomain.com/fullchain.pem;
    ssl_certificate_key /etc/nginx/certs/live/yourdomain.com/privkey.pem;

    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;

    location / {
        proxy_pass http://app:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }
}
```

### Environment Variables

```env
# .env.example

# Database
DATABASE_URL="postgresql://postgres:password@db:5432/quilltap"

# NextAuth
NEXTAUTH_URL="http://localhost:3000"  # Production: https://yourdomain.com
NEXTAUTH_SECRET="generate-with-openssl-rand-base64-32"

# OAuth Providers
GOOGLE_CLIENT_ID="your-google-client-id"
GOOGLE_CLIENT_SECRET="your-google-client-secret"

# Post-1.0:
# APPLE_ID=""
# APPLE_SECRET=""
# GITHUB_ID=""
# GITHUB_SECRET=""

# Encryption
ENCRYPTION_MASTER_PEPPER="generate-with-openssl-rand-base64-32"

# Production SSL (optional for local dev)
DOMAIN="yourdomain.com"
SSL_EMAIL="admin@yourdomain.com"
```

---

## LLM Provider Integration

### Abstract Interface

```typescript
// lib/llm/base.ts
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface LLMParams {
  messages: LLMMessage[]
  model: string
  temperature?: number
  maxTokens?: number
  topP?: number
  stop?: string[]
}

export interface LLMResponse {
  content: string
  finishReason: string
  usage: {
    promptTokens: number
    completionTokens: number
    totalTokens: number
  }
  raw: any // Provider-specific raw response
}

export abstract class LLMProvider {
  abstract sendMessage(params: LLMParams): Promise<LLMResponse>
  abstract streamMessage(params: LLMParams): AsyncGenerator<string>
  abstract validateApiKey(apiKey: string): Promise<boolean>
  abstract getAvailableModels(apiKey: string): Promise<string[]>
}
```

### Provider Implementations

#### OpenAI

```typescript
// lib/llm/openai.ts
import OpenAI from 'openai'
import { LLMProvider, LLMParams, LLMResponse } from './base'

export class OpenAIProvider extends LLMProvider {
  async sendMessage(params: LLMParams, apiKey: string): Promise<LLMResponse> {
    const client = new OpenAI({ apiKey })

    const response = await client.chat.completions.create({
      model: params.model,
      messages: params.messages,
      temperature: params.temperature ?? 0.7,
      max_tokens: params.maxTokens ?? 1000,
      top_p: params.topP ?? 1,
      stop: params.stop,
    })

    const choice = response.choices[0]

    return {
      content: choice.message.content ?? '',
      finishReason: choice.finish_reason,
      usage: {
        promptTokens: response.usage?.prompt_tokens ?? 0,
        completionTokens: response.usage?.completion_tokens ?? 0,
        totalTokens: response.usage?.total_tokens ?? 0,
      },
      raw: response,
    }
  }

  async *streamMessage(params: LLMParams, apiKey: string) {
    const client = new OpenAI({ apiKey })

    const stream = await client.chat.completions.create({
      model: params.model,
      messages: params.messages,
      temperature: params.temperature ?? 0.7,
      max_tokens: params.maxTokens ?? 1000,
      stream: true,
    })

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content
      if (content) yield content
    }
  }

  async validateApiKey(apiKey: string): Promise<boolean> {
    try {
      const client = new OpenAI({ apiKey })
      await client.models.list()
      return true
    } catch {
      return false
    }
  }

  async getAvailableModels(apiKey: string): Promise<string[]> {
    const client = new OpenAI({ apiKey })
    const models = await client.models.list()
    return models.data
      .filter(m => m.id.includes('gpt'))
      .map(m => m.id)
  }
}
```

#### Anthropic

```typescript
// lib/llm/anthropic.ts
import Anthropic from '@anthropic-ai/sdk'
import { LLMProvider, LLMParams, LLMResponse } from './base'

export class AnthropicProvider extends LLMProvider {
  async sendMessage(params: LLMParams, apiKey: string): Promise<LLMResponse> {
    const client = new Anthropic({ apiKey })

    // Anthropic requires system message separate
    const systemMessage = params.messages.find(m => m.role === 'system')
    const messages = params.messages.filter(m => m.role !== 'system')

    const response = await client.messages.create({
      model: params.model,
      system: systemMessage?.content,
      messages: messages as any,
      max_tokens: params.maxTokens ?? 1000,
      temperature: params.temperature ?? 0.7,
      top_p: params.topP ?? 1,
    })

    const content = response.content[0]

    return {
      content: content.type === 'text' ? content.text : '',
      finishReason: response.stop_reason ?? 'stop',
      usage: {
        promptTokens: response.usage.input_tokens,
        completionTokens: response.usage.output_tokens,
        totalTokens: response.usage.input_tokens + response.usage.output_tokens,
      },
      raw: response,
    }
  }

  async *streamMessage(params: LLMParams, apiKey: string) {
    const client = new Anthropic({ apiKey })

    const systemMessage = params.messages.find(m => m.role === 'system')
    const messages = params.messages.filter(m => m.role !== 'system')

    const stream = await client.messages.create({
      model: params.model,
      system: systemMessage?.content,
      messages: messages as any,
      max_tokens: params.maxTokens ?? 1000,
      stream: true,
    })

    for await (const event of stream) {
      if (event.type === 'content_block_delta' &&
          event.delta.type === 'text_delta') {
        yield event.delta.text
      }
    }
  }

  async validateApiKey(apiKey: string): Promise<boolean> {
    try {
      const client = new Anthropic({ apiKey })
      await client.messages.create({
        model: 'claude-3-haiku-20240307',
        max_tokens: 1,
        messages: [{ role: 'user', content: 'test' }],
      })
      return true
    } catch {
      return false
    }
  }

  async getAvailableModels(): Promise<string[]> {
    // Anthropic doesn't have a models endpoint, return known models
    return [
      'claude-3-5-sonnet-20241022',
      'claude-3-opus-20240229',
      'claude-3-sonnet-20240229',
      'claude-3-haiku-20240307',
    ]
  }
}
```

#### Ollama

```typescript
// lib/llm/ollama.ts
import { LLMProvider, LLMParams, LLMResponse } from './base'

export class OllamaProvider extends LLMProvider {
  constructor(private baseUrl: string) {
    super()
  }

  async sendMessage(params: LLMParams): Promise<LLMResponse> {
    const response = await fetch(`${this.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: params.model,
        messages: params.messages,
        stream: false,
        options: {
          temperature: params.temperature ?? 0.7,
          num_predict: params.maxTokens ?? 1000,
          top_p: params.topP ?? 1,
          stop: params.stop,
        },
      }),
    })

    const data = await response.json()

    return {
      content: data.message.content,
      finishReason: data.done ? 'stop' : 'length',
      usage: {
        promptTokens: data.prompt_eval_count ?? 0,
        completionTokens: data.eval_count ?? 0,
        totalTokens: (data.prompt_eval_count ?? 0) + (data.eval_count ?? 0),
      },
      raw: data,
    }
  }

  async *streamMessage(params: LLMParams) {
    const response = await fetch(`${this.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: params.model,
        messages: params.messages,
        stream: true,
        options: {
          temperature: params.temperature ?? 0.7,
          num_predict: params.maxTokens ?? 1000,
        },
      }),
    })

    const reader = response.body?.getReader()
    const decoder = new TextDecoder()

    while (reader) {
      const { done, value } = await reader.read()
      if (done) break

      const chunk = decoder.decode(value)
      const lines = chunk.split('\n').filter(Boolean)

      for (const line of lines) {
        const data = JSON.parse(line)
        if (data.message?.content) {
          yield data.message.content
        }
      }
    }
  }

  async validateApiKey(): Promise<boolean> {
    // Ollama doesn't use API keys, just check if server is reachable
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`)
      return response.ok
    } catch {
      return false
    }
  }

  async getAvailableModels(): Promise<string[]> {
    const response = await fetch(`${this.baseUrl}/api/tags`)
    const data = await response.json()
    return data.models?.map((m: any) => m.name) ?? []
  }
}
```

#### OpenRouter

```typescript
// lib/llm/openrouter.ts
import { OpenAIProvider } from './openai'

export class OpenRouterProvider extends OpenAIProvider {
  // OpenRouter is OpenAI-compatible, just different base URL
  constructor() {
    super()
  }

  async sendMessage(params: LLMParams, apiKey: string) {
    // Use OpenAI client with custom base URL
    const OpenAI = require('openai')
    const client = new OpenAI({
      apiKey,
      baseURL: 'https://openrouter.ai/api/v1',
    })

    // Rest is same as OpenAI implementation
    // ... (reuse parent logic with custom client)
  }

  async getAvailableModels(apiKey: string): Promise<string[]> {
    const response = await fetch('https://openrouter.ai/api/v1/models', {
      headers: { 'Authorization': `Bearer ${apiKey}` },
    })
    const data = await response.json()
    return data.data?.map((m: any) => m.id) ?? []
  }
}
```

#### OpenAI-Compatible (Generic)

```typescript
// lib/llm/openai-compatible.ts
import OpenAI from 'openai'
import { LLMProvider, LLMParams, LLMResponse } from './base'

export class OpenAICompatibleProvider extends LLMProvider {
  constructor(private baseUrl: string) {
    super()
  }

  async sendMessage(params: LLMParams, apiKey: string): Promise<LLMResponse> {
    const client = new OpenAI({
      apiKey: apiKey || 'not-needed', // Some don't require keys
      baseURL: this.baseUrl,
    })

    // Same implementation as OpenAI
    // ... (reuse OpenAI logic)
  }

  // ... rest same as OpenAI
}
```

### Provider Factory

```typescript
// lib/llm/factory.ts
import { Provider } from '@prisma/client'
import { LLMProvider } from './base'
import { OpenAIProvider } from './openai'
import { AnthropicProvider } from './anthropic'
import { OllamaProvider } from './ollama'
import { OpenRouterProvider } from './openrouter'
import { OpenAICompatibleProvider } from './openai-compatible'

export function createLLMProvider(
  provider: Provider,
  baseUrl?: string
): LLMProvider {
  switch (provider) {
    case 'OPENAI':
      return new OpenAIProvider()
    case 'ANTHROPIC':
      return new AnthropicProvider()
    case 'OLLAMA':
      if (!baseUrl) throw new Error('Ollama requires baseUrl')
      return new OllamaProvider(baseUrl)
    case 'OPENROUTER':
      return new OpenRouterProvider()
    case 'OPENAI_COMPATIBLE':
      if (!baseUrl) throw new Error('OpenAI-compatible requires baseUrl')
      return new OpenAICompatibleProvider(baseUrl)
    default:
      throw new Error(`Unsupported provider: ${provider}`)
  }
}
```

---

## Chat Initialization System

### Context Building

```typescript
// lib/chat/initialize.ts
import { prisma } from '@/lib/prisma'

export async function buildChatContext(
  characterId: string,
  personaId?: string,
  customScenario?: string
) {
  const character = await prisma.character.findUnique({
    where: { id: characterId },
    include: {
      personas: {
        where: personaId ? { personaId } : { isDefault: true },
        include: { persona: true },
      },
    },
  })

  if (!character) throw new Error('Character not found')

  const persona = personaId
    ? await prisma.persona.findUnique({ where: { id: personaId } })
    : character.personas[0]?.persona

  // Build system prompt
  const systemPrompt = buildSystemPrompt({
    character,
    persona,
    scenario: customScenario || character.scenario,
  })

  // Build first message
  const firstMessage = character.firstMessage

  return {
    systemPrompt,
    firstMessage,
    character,
    persona,
  }
}

function buildSystemPrompt({
  character,
  persona,
  scenario,
}: {
  character: any
  persona?: any
  scenario: string
}) {
  let prompt = character.systemPrompt || ''

  // Add character identity
  prompt += `\n\nYou are roleplaying as ${character.name}.`

  // Add character description
  if (character.description) {
    prompt += `\n\nCharacter Description:\n${character.description}`
  }

  // Add personality
  if (character.personality) {
    prompt += `\n\nPersonality:\n${character.personality}`
  }

  // Add persona (who they're talking to)
  if (persona) {
    prompt += `\n\nYou are talking to ${persona.name}.`
    if (persona.description) {
      prompt += `\n${persona.description}`
    }
    if (persona.personalityTraits) {
      prompt += `\nThey are: ${persona.personalityTraits}`
    }
  }

  // Add scenario
  if (scenario) {
    prompt += `\n\nScenario:\n${scenario}`
  }

  // Add example dialogues
  if (character.exampleDialogues) {
    prompt += `\n\nExample Dialogue:\n${character.exampleDialogues}`
  }

  // Add roleplay instructions
  prompt += `\n\nStay in character at all times. Respond naturally and consistently with ${character.name}'s personality and the current scenario.`

  return prompt.trim()
}
```

### Chat Creation API

```typescript
// app/api/chats/route.ts
import { NextRequest, NextResponse } from 'next/server'
import { getServerSession } from 'next-auth'
import { authOptions } from '@/app/api/auth/[...nextauth]/route'
import { prisma } from '@/lib/prisma'
import { buildChatContext } from '@/lib/chat/initialize'

export async function POST(req: NextRequest) {
  const session = await getServerSession(authOptions)
  if (!session?.user?.id) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
  }

  const body = await req.json()
  const { characterId, personaId, connectionProfileId, scenario, title } = body

  // Build context
  const context = await buildChatContext(characterId, personaId, scenario)

  // Create chat
  const chat = await prisma.chat.create({
    data: {
      userId: session.user.id,
      characterId,
      personaId,
      connectionProfileId,
      title: title || `Chat with ${context.character.name}`,
      contextSummary: scenario,
    },
  })

  // Create system message
  await prisma.message.create({
    data: {
      chatId: chat.id,
      role: 'SYSTEM',
      content: context.systemPrompt,
    },
  })

  // Create first message from character
  await prisma.message.create({
    data: {
      chatId: chat.id,
      role: 'ASSISTANT',
      content: context.firstMessage,
    },
  })

  return NextResponse.json(chat)
}
```

---

## SillyTavern Compatibility

### Character Import/Export

```typescript
// lib/sillytavern/character.ts

export interface STCharacterV2 {
  name: string
  description: string
  personality: string
  scenario: string
  first_mes: string
  mes_example: string
  creator_notes?: string
  system_prompt?: string
  post_history_instructions?: string
  tags?: string[]
  creator?: string
  character_version?: string
  extensions?: Record<string, any>
}

export function importSTCharacter(stData: STCharacterV2) {
  return {
    name: stData.name,
    description: stData.description,
    personality: stData.personality,
    scenario: stData.scenario,
    firstMessage: stData.first_mes,
    exampleDialogues: stData.mes_example,
    systemPrompt: stData.system_prompt || '',
    sillyTavernData: stData, // Store original for full fidelity
  }
}

export function exportSTCharacter(character: any): STCharacterV2 {
  // If we have original ST data, use it as base
  if (character.sillyTavernData) {
    return {
      ...character.sillyTavernData,
      // Override with current values
      name: character.name,
      description: character.description,
      personality: character.personality,
      scenario: character.scenario,
      first_mes: character.firstMessage,
      mes_example: character.exampleDialogues || '',
      system_prompt: character.systemPrompt || '',
    }
  }

  // Create new ST format
  return {
    name: character.name,
    description: character.description,
    personality: character.personality,
    scenario: character.scenario,
    first_mes: character.firstMessage,
    mes_example: character.exampleDialogues || '',
    system_prompt: character.systemPrompt || '',
    creator_notes: '',
    tags: [],
    creator: 'Quilltap',
    character_version: '1.0',
    extensions: {},
  }
}
```

### Chat Import/Export

```typescript
// lib/sillytavern/chat.ts

export interface STMessage {
  name: string
  is_user: boolean
  is_name: boolean
  send_date: number
  mes: string
  swipes?: string[]
  swipe_id?: number
}

export interface STChat {
  messages: STMessage[]
  chat_metadata?: {
    note_prompt?: string
    note_interval?: number
    [key: string]: any
  }
}

export function importSTChat(stChat: STChat, characterId: string, userId: string) {
  const messages = stChat.messages.map((msg, index) => ({
    role: msg.is_user ? 'USER' : 'ASSISTANT',
    content: msg.mes,
    swipeGroupId: msg.swipes ? `swipe-${index}` : null,
    swipeIndex: msg.swipe_id ?? 0,
    createdAt: new Date(msg.send_date),
  }))

  return {
    messages,
    metadata: stChat.chat_metadata,
  }
}

export function exportSTChat(chat: any, messages: any[]): STChat {
  const stMessages: STMessage[] = messages
    .filter(m => m.role !== 'SYSTEM')
    .map(msg => ({
      name: msg.role === 'USER' ? 'User' : chat.character.name,
      is_user: msg.role === 'USER',
      is_name: true,
      send_date: msg.createdAt.getTime(),
      mes: msg.content,
      // Handle swipes if present
      ...(msg.swipeGroupId && {
        swipes: messages
          .filter(m => m.swipeGroupId === msg.swipeGroupId)
          .map(m => m.content),
        swipe_id: msg.swipeIndex,
      }),
    }))

  return {
    messages: stMessages,
    chat_metadata: chat.sillyTavernMetadata || {},
  }
}
```

### Persona Import/Export

```typescript
// lib/sillytavern/persona.ts

export interface STPersona {
  name: string
  description: string
  personality?: string
  scenario?: string
  mes_example?: string
  [key: string]: any
}

export function importSTPersona(stData: STPersona) {
  return {
    name: stData.name,
    description: stData.description,
    personalityTraits: stData.personality || '',
    sillyTavernData: stData,
  }
}

export function exportSTPersona(persona: any): STPersona {
  if (persona.sillyTavernData) {
    return {
      ...persona.sillyTavernData,
      name: persona.name,
      description: persona.description,
      personality: persona.personalityTraits || '',
    }
  }

  return {
    name: persona.name,
    description: persona.description,
    personality: persona.personalityTraits || '',
  }
}
```

---

## Testing Strategy

### Unit Tests (Jest)

```typescript
// __tests__/unit/encryption.test.ts
import { encryptApiKey, decryptApiKey } from '@/lib/encryption'

describe('Encryption', () => {
  const userId = 'test-user-id'
  const apiKey = 'sk-test-api-key-12345'

  it('should encrypt and decrypt API key', () => {
    const encrypted = encryptApiKey(apiKey, userId)
    const decrypted = decryptApiKey(
      encrypted.encrypted,
      encrypted.iv,
      encrypted.authTag,
      userId
    )

    expect(decrypted).toBe(apiKey)
  })

  it('should fail with wrong user ID', () => {
    const encrypted = encryptApiKey(apiKey, userId)

    expect(() => {
      decryptApiKey(
        encrypted.encrypted,
        encrypted.iv,
        encrypted.authTag,
        'wrong-user-id'
      )
    }).toThrow()
  })

  it('should use different IVs for same key', () => {
    const enc1 = encryptApiKey(apiKey, userId)
    const enc2 = encryptApiKey(apiKey, userId)

    expect(enc1.iv).not.toBe(enc2.iv)
    expect(enc1.encrypted).not.toBe(enc2.encrypted)
  })
})
```

### Integration Tests (Playwright)

```typescript
// __tests__/integration/chat-flow.test.ts
import { test, expect } from '@playwright/test'

test.describe('Chat Flow', () => {
  test.beforeEach(async ({ page }) => {
    // Sign in
    await page.goto('/signin')
    await page.click('button:has-text("Sign in with Google")')
    // ... OAuth flow
  })

  test('should create character and start chat', async ({ page }) => {
    // Create character
    await page.goto('/characters/new')
    await page.fill('input[name="name"]', 'Test Character')
    await page.fill('textarea[name="description"]', 'A test character')
    await page.fill('textarea[name="personality"]', 'Friendly and helpful')
    await page.fill('textarea[name="scenario"]', 'Testing scenario')
    await page.fill('textarea[name="firstMessage"]', 'Hello, I am Test Character!')
    await page.click('button:has-text("Create")')

    // Start chat
    await expect(page).toHaveURL(/\/characters\/[a-z0-9-]+/)
    await page.click('button:has-text("Start Chat")')

    // Verify chat initialized
    await expect(page).toHaveURL(/\/chats\/[a-z0-9-]+/)
    await expect(page.locator('text=Hello, I am Test Character!')).toBeVisible()

    // Send message
    await page.fill('textarea[placeholder="Type a message..."]', 'Hi there!')
    await page.click('button:has-text("Send")')

    // Verify response (mocked in test)
    await expect(page.locator('.message.assistant')).toBeVisible()
  })
})
```

### Docker Test Environment

```dockerfile
# Dockerfile.test
FROM node:20-alpine AS test

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .

# Run tests
RUN npm run test:unit
RUN npm run test:integration

CMD ["npm", "test"]
```

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  test:
    build:
      context: .
      dockerfile: Dockerfile.test
    environment:
      DATABASE_URL: postgresql://postgres:test@test-db:5432/quilltap_test
      NEXTAUTH_SECRET: test-secret
      ENCRYPTION_MASTER_PEPPER: test-pepper
    depends_on:
      - test-db

  test-db:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: quilltap_test
      POSTGRES_PASSWORD: test
```

### Test Scripts

```json
{
  "scripts": {
    "test": "jest",
    "test:unit": "jest --testPathPattern=__tests__/unit",
    "test:integration": "jest --testPathPattern=__tests__/integration",
    "test:api": "jest --testPathPattern=__tests__/api",
    "test:e2e": "playwright test",
    "test:docker": "docker-compose -f docker-compose.test.yml up --abort-on-container-exit",
    "test:coverage": "jest --coverage",
    "test:watch": "jest --watch"
  }
}
```

---

## Docker Configuration

### Development Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - DATABASE_URL=postgresql://postgres:dev_password@db:5432/quilltap
      - NEXTAUTH_URL=http://localhost:3000
    env_file:
      - .env.local
    volumes:
      - .:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      db:
        condition: service_healthy
    command: npm run dev

  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: quilltap
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: dev_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_dev:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

volumes:
  postgres_dev:
```

### Production Docker Compose

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./certs:/etc/nginx/certs:ro
      - ./certbot/www:/var/www/certbot:ro
    depends_on:
      - app
    restart: unless-stopped

  certbot:
    image: certbot/certbot
    volumes:
      - ./certs:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"

  app:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    expose:
      - "3000"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@db:5432/${DB_NAME}
    env_file:
      - .env.production
    depends_on:
      db:
        condition: service_healthy
    restart: unless-stopped

  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_prod:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

volumes:
  postgres_prod:
```

### Multi-Stage Dockerfile

```dockerfile
# Dockerfile
FROM node:20-alpine AS base

# Install dependencies only when needed
FROM base AS deps
WORKDIR /app

COPY package.json package-lock.json ./
RUN npm ci

# Development stage
FROM base AS development
WORKDIR /app

COPY --from=deps /app/node_modules ./node_modules
COPY . .

EXPOSE 3000
CMD ["npm", "run", "dev"]

# Build stage
FROM base AS builder
WORKDIR /app

COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Generate Prisma client
RUN npx prisma generate

# Build Next.js
RUN npm run build

# Production stage
FROM base AS production
WORKDIR /app

ENV NODE_ENV=production

# Create non-root user
RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs

# Copy built assets
COPY --from=builder /app/public ./public
COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static

USER nextjs

EXPOSE 3000

ENV PORT 3000
ENV HOSTNAME "0.0.0.0"

CMD ["node", "server.js"]
```

### Next.js Config for Docker

```javascript
// next.config.js
/** @type {import('next').NextConfig} */
const nextConfig = {
  output: 'standalone', // For Docker production builds
  experimental: {
    serverComponentsExternalPackages: ['@prisma/client'],
  },
}

module.exports = nextConfig
```

---

## Project File Structure

```text
quilltap/
├── app/
│   ├── api/
│   │   ├── auth/[...nextauth]/route.ts
│   │   ├── keys/
│   │   ├── profiles/
│   │   ├── characters/
│   │   ├── personas/
│   │   ├── chats/
│   │   └── messages/
│   ├── (dashboard)/
│   │   ├── layout.tsx
│   │   ├── page.tsx
│   │   ├── characters/
│   │   ├── personas/
│   │   ├── chats/
│   │   └── settings/
│   ├── (auth)/
│   │   ├── signin/page.tsx
│   │   └── error/page.tsx
│   ├── layout.tsx
│   └── globals.css
├── components/
│   ├── ui/                  # shadcn/ui components
│   ├── chat/
│   ├── character/
│   └── layout/
├── lib/
│   ├── prisma.ts
│   ├── encryption.ts
│   ├── llm/
│   │   ├── base.ts
│   │   ├── openai.ts
│   │   ├── anthropic.ts
│   │   ├── ollama.ts
│   │   ├── openrouter.ts
│   │   ├── openai-compatible.ts
│   │   └── factory.ts
│   ├── chat/
│   │   └── initialize.ts
│   ├── sillytavern/
│   │   ├── character.ts
│   │   ├── persona.ts
│   │   └── chat.ts
│   └── utils.ts
├── prisma/
│   ├── schema.prisma
│   └── migrations/
├── __tests__/
│   ├── unit/
│   ├── integration/
│   └── api/
├── docker/
│   ├── nginx.conf
│   └── init-letsencrypt.sh
├── public/
├── features/
│   └── ROADMAP.md          # This file
├── .env.example
├── .env.local
├── .env.production
├── .gitignore
├── docker-compose.yml
├── docker-compose.prod.yml
├── docker-compose.test.yml
├── Dockerfile
├── Dockerfile.test
├── package.json
├── tsconfig.json
├── tailwind.config.ts
├── jest.config.js
├── playwright.config.ts
└── README.md
```

---

## Deployment Guide

### Prerequisites

1. VPS or cloud server (2GB+ RAM, 20GB+ disk)
2. Domain name with DNS configured
3. Google OAuth credentials
4. LLM provider API keys (for testing)

### Initial Setup

```bash
# 1. Clone repository
git clone https://github.com/yourusername/quilltap.git
cd quilltap

# 2. Create .env.production
cp .env.example .env.production
# Edit .env.production with your values

# 3. Initialize SSL certificates
chmod +x docker/init-letsencrypt.sh
./docker/init-letsencrypt.sh yourdomain.com admin@yourdomain.com

# 4. Build and start services
docker-compose -f docker-compose.prod.yml up -d

# 5. Run database migrations
docker-compose -f docker-compose.prod.yml exec app npx prisma migrate deploy

# 6. Check logs
docker-compose -f docker-compose.prod.yml logs -f
```

### SSL Certificate Renewal

- Automatic via Certbot (runs every 12 hours)
- Verify: `docker-compose -f docker-compose.prod.yml logs certbot`

### Database Backups

```bash
# Backup script (run via cron)
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
docker-compose -f docker-compose.prod.yml exec -T db \
  pg_dump -U $DB_USER $DB_NAME | \
  gzip > backups/quilltap_$DATE.sql.gz

# Keep only last 7 days
find backups/ -name "quilltap_*.sql.gz" -mtime +7 -delete
```

### Updates

```bash
# Pull latest code
git pull origin main

# Rebuild and restart
docker-compose -f docker-compose.prod.yml up -d --build

# Run migrations if needed
docker-compose -f docker-compose.prod.yml exec app npx prisma migrate deploy
```

---

## Cost Estimates (Monthly)

### Infrastructure Costs

- **VPS** (Hetzner/DigitalOcean): $10-20/month (4GB RAM)
- **Domain**: $1-2/month
- **SSL**: Free (Let's Encrypt)
- **Total Infrastructure**: ~$12-22/month

### LLM Costs

- Users bring their own API keys
- No direct cost to you

### Scaling Costs

- **100 active users**: Same infrastructure (~$12-22/month)
- **1,000 active users**: $40-60/month (better VPS)
- **10,000 active users**: $200-500/month (load balancer + multiple app servers)

---

## Security Checklist

- [x] Per-user encryption for API keys
- [x] OAuth for authentication (no password storage)
- [x] JWT in httpOnly cookies
- [x] SSL/TLS in production
- [ ] Rate limiting on API endpoints
- [ ] Input validation (Zod schemas)
- [ ] SQL injection prevention (Prisma ORM)
- [ ] XSS prevention (React escaping)
- [ ] CSRF protection (Next.js built-in)
- [ ] Environment variable validation
- [ ] Secure headers (Helmet.js)
- [ ] API key masking in UI
- [ ] Audit logging for sensitive operations
- [ ] Regular dependency updates
- [ ] Security testing in CI/CD

---

## FAQ & Troubleshooting

### Q: Why Next.js instead of separate frontend/backend?

**A**: Faster development, single codebase, no CORS issues, better DX, easier deployment.

### Q: Do I need Redis for v1.0?

**A**: No. Add it later if you need caching or rate limiting across multiple servers.

### Q: Can I use a different OAuth provider first?

**A**: Yes, but Google has the easiest setup. NextAuth supports 50+ providers.

### Q: How secure is the API key encryption?

**A**: Very secure. AES-256-GCM with per-user keys. User must authenticate to decrypt.

### Q: What if I lose the ENCRYPTION_MASTER_PEPPER?

**A**: All API keys become unrecoverable. **Back up this value securely.**

### Q: Can users share characters?

**A**: Not in v1.0, but easy to add in v1.1 (add `isPublic` flag to characters).

### Q: Will this work on ARM servers (like Oracle Cloud free tier)?

**A**: Yes, Docker images are multi-arch. Tested on ARM64.

### Q: How to migrate from SillyTavern?

**A**: Import characters, personas, and chats via the UI. Full compatibility.

---

## Next Steps

Once roadmap is approved:

1. **Initialize project**: Create Next.js app with all configurations
2. **Set up Docker**: Development environment with PostgreSQL
3. **Implement OAuth**: Get Google auth working
4. **Build encryption**: Test API key storage
5. **Create first UI**: Dashboard and settings pages
6. **Iterate rapidly**: Follow phase milestones

**Estimated time to v1.0**: 10-11 weeks (aggressive) or 14-16 weeks (comfortable pace)

---

**Project Status**: Planning Phase
**Target v1.0 Release**: TBD
**Maintainer**: Foundry-9
