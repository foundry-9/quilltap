{
  "$schema": "../qtap-plugin-template/schemas/plugin-manifest.schema.json",
  "name": "qtap-plugin-ollama",
  "title": "Ollama Provider",
  "description": "Provides local Ollama LLM models and embeddings for Quilltap - run AI models on your own machine",
  "version": "1.0.0",
  "author": {
    "name": "Foundry-9",
    "email": "charles@sebold.tech",
    "url": "https://foundry-9.com"
  },
  "license": "MIT",
  "compatibility": {
    "quilltapVersion": ">=1.7.0",
    "nodeVersion": ">=18.0.0"
  },
  "capabilities": ["LLM_PROVIDER"],
  "category": "PROVIDER",
  "main": "index.js",
  "typescript": true,
  "frontend": "REACT",
  "styling": "TAILWIND",
  "enabledByDefault": true,
  "status": "STABLE",
  "keywords": ["ollama", "local", "offline", "llm", "llava", "neural-chat", "mistral", "ai", "chat"],
  "providerConfig": {
    "providerName": "OLLAMA",
    "displayName": "Ollama",
    "description": "Local Ollama LLM models for offline AI inference",
    "abbreviation": "OLL",
    "colors": {
      "bg": "bg-gray-100",
      "text": "text-gray-800",
      "icon": "text-gray-600"
    },
    "requiresApiKey": false,
    "requiresBaseUrl": true,
    "baseUrlLabel": "Ollama Base URL",
    "baseUrlDefault": "http://localhost:11434",
    "capabilities": {
      "chat": true,
      "imageGeneration": false,
      "embeddings": true,
      "webSearch": false
    },
    "attachmentSupport": {
      "supported": false,
      "mimeTypes": [],
      "description": "File attachments not yet supported (requires multimodal model detection)"
    }
  },
  "permissions": {
    "network": ["localhost:11434"],
    "userData": false,
    "database": false
  }
}
